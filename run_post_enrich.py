# run_post_enrich.py
"""
Targeted post-enricher

Usage:
  python run_post_enrich.py --min-score 70 --concurrency 30

What it does:
 - Loads ./store/iocs_indexed.json (generated by run_index.py).
 - Selects IOCs with score >= --min-score OR risk_bucket == --bucket (if provided).
 - Skips entries already containing deep fields in the cache (WHOIS / OTX / AbuseIPDB).
 - Performs local enrichment (DNS/geo/reverse) + WHOIS + HTTP API lookups (OTX for domains, AbuseIPDB for IPs).
 - Updates ./store/enrich_cache.json and appends results to ./store/iocs_enriched.jsonl (incremental).
 - Writes aggregated ./store/iocs_enriched.json at end.
"""

import os
import json
import argparse
import asyncio
from functools import partial
import aiohttp
from collectors.enrich import enrich_local
from utils.config import Config

INDEX_FILE = "./store/iocs_indexed.json"
CACHE_FILE = "./store/enrich_cache.json"
OUT_JSONL = "./store/iocs_enriched.jsonl"
OUT_JSON = "./store/iocs_enriched.json"

# ---------------- cache helpers ----------------
def load_cache():
    if os.path.exists(CACHE_FILE):
        try:
            with open(CACHE_FILE, "r") as f:
                return json.load(f)
        except Exception:
            return {}
    return {}

def save_cache(cache):
    tmp = CACHE_FILE + ".tmp"
    with open(tmp, "w") as f:
        json.dump(cache, f, indent=2)
    os.replace(tmp, CACHE_FILE)

# --------------- HTTP helpers -------------------
async def http_otx_lookup(session, domain):
    key = Config.OTX_API_KEY
    if not key:
        return {}
    url = f"https://otx.alienvault.com/api/v1/indicators/domain/{domain}/general"
    headers = {"X-OTX-API-KEY": key}
    try:
        async with session.get(url, headers=headers, timeout=15) as resp:
            if resp.status == 200:
                return await resp.json()
    except Exception:
        return {}
    return {}

async def http_abuseipdb_lookup(session, ip):
    key = Config.ABUSEIPDB_API_KEY
    if not key:
        return {}
    url = "https://api.abuseipdb.com/api/v2/check"
    params = {"ipAddress": ip, "maxAgeInDays": 90}
    headers = {"Key": key, "Accept": "application/json"}
    try:
        async with session.get(url, params=params, headers=headers, timeout=15) as resp:
            if resp.status == 200:
                return await resp.json()
    except Exception:
        return {}
    return {}

# ---------- decide if item needs deep enrichment ----------
def needs_deep_enrich(cache_entry, typ):
    """
    Return True if cache_entry lacks deep fields that we want to fetch now.
    For domains: want whois and otx.
    For IP: want abuseipdb.
    If cache_entry is None -> True
    """
    if not cache_entry:
        return True
    ench = cache_entry.get("enrichment", {}) or {}
    if typ == "domain":
        whois = ench.get("whois") or {}
        otx = ench.get("otx") or ench.get("otx_count")
        # perform deep enrich if whois is empty or otx missing
        if not whois or not otx:
            return True
        return False
    if typ == "ip":
        if ench.get("abuseipdb") or ench.get("abuseipdb_score"):
            return False
        return True
    # default: do deep enrich
    return True

# ------------- worker ----------------------------
async def enrich_target_worker(ioc, idx, total, session, sem, cache, progress_queue):
    key = f"{ioc.get('type')}::{ioc.get('value')}"
    if key in cache and not needs_deep_enrich(cache[key], ioc.get("type")):
        if progress_queue:
            await progress_queue.put({"idx": idx, "total": total, "id": key, "step": "SKIP", "status": "cached"})
        return cache[key]

    async with sem:
        # first do local enrichment (DNS + whois if not skipped by enrich_local)
        # We want whois now, so call enrich_local with skip_whois=False
        local = await enrich_local(ioc, progress_queue=progress_queue, skip_whois=False)
        enrichment = local.get("enrichment", {}) or {}

        # now do HTTP enrich depending on type
        if ioc.get("type") == "domain":
            otx = await http_otx_lookup(session, ioc.get("value"))
            if otx:
                enrichment["otx"] = {"raw": otx, "count": otx.get("pulse_info", {}).get("count") or otx.get("pulse_info", {}).get("indicator_count") or 0}
        elif ioc.get("type") == "ip":
            abuse = await http_abuseipdb_lookup(session, ioc.get("value"))
            if abuse:
                enrichment["abuseipdb"] = abuse.get("data") or abuse
                enrichment["abuseipdb_score"] = (abuse.get("data") or {}).get("abuseConfidenceScore") or (abuse.get("data") or {}).get("abuseConfidence") or enrichment.get("abuseipdb_score")

        result = {
            "type": ioc.get("type"),
            "value": ioc.get("value"),
            "source": ioc.get("source"),
            "first_seen": ioc.get("first_seen"),
            "last_seen": ioc.get("last_seen"),
            "sources_count": ioc.get("sources_count", 1),
            "enrichment": enrichment
        }

        cache[key] = result

        # write incremental jsonl entry
        try:
            with open(OUT_JSONL, "a") as f:
                f.write(json.dumps(result) + "\n")
        except Exception:
            pass

        if progress_queue:
            await progress_queue.put({"idx": idx, "total": total, "id": key, "step": "COMPLETE", "status": "done"})

        return result

# ------------- progress printer --------------------
async def progress_printer(queue):
    while True:
        msg = await queue.get()
        if msg is None:
            queue.task_done()
            break
        idx = msg.get("idx")
        total = msg.get("total")
        key = msg.get("id")
        step = msg.get("step")
        status = msg.get("status")
        mark = "âœ“" if status in ("done","cached") else status
        if idx and total:
            print(f"[{idx:4d}/{total}] {key} - {step} {mark}")
        else:
            print(f"[    ] {key} - {step} {mark}")
        queue.task_done()

# ------------- main async flow ---------------------
async def main_async(min_score, bucket, concurrency, limit):
    if not os.path.exists(INDEX_FILE):
        print("Index file not found. Run `python run_index.py` first.")
        return

    with open(INDEX_FILE, "r") as f:
        indexed = json.load(f)

    # pick items by score or bucket
    selected = []
    for e in indexed:
        if bucket:
            if e.get("risk_bucket") == bucket:
                selected.append(e)
        else:
            if e.get("score", 0) >= min_score:
                selected.append(e)

    if limit:
        selected = selected[:limit]

    total = len(selected)
    print(f"Selected {total} IOCs for deep enrichment (min_score={min_score}, bucket={bucket})")

    cache = load_cache()

    # make output dirs and files
    os.makedirs("./store", exist_ok=True)
    if not os.path.exists(OUT_JSONL):
        open(OUT_JSONL, "a").close()

    sem = asyncio.Semaphore(concurrency)
    progress_q = asyncio.Queue()
    printer = asyncio.create_task(progress_printer(progress_q))

    timeout = aiohttp.ClientTimeout(total=30)
    async with aiohttp.ClientSession(timeout=timeout) as session:
        tasks = []
        for idx, e in enumerate(selected, start=1):
            ioc = {
                "type": e.get("type"),
                "value": e.get("value"),
                "source": e.get("source"),
                "first_seen": e.get("first_seen"),
                "last_seen": e.get("last_seen"),
                "sources_count": e.get("sources_count", 1)
            }
            tasks.append(asyncio.create_task(enrich_target_worker(ioc, idx, total, session, sem, cache, progress_q)))

        # gather in batches to avoid mem spikes
        BATCH = 50
        results = []
        for i in range(0, len(tasks), BATCH):
            chunk = tasks[i:i+BATCH]
            chunk_res = await asyncio.gather(*chunk)
            results.extend(chunk_res)
            # persist cache incremental
            save_cache(cache)
            print(f"Completed chunk {i//BATCH + 1} ({len(results)}/{total})")

    # finalize aggregated enriched JSON from cache
    try:
        with open(OUT_JSON, "w") as f:
            json.dump(list(cache.values()), f, indent=2)
        print(f"Wrote aggregated enriched file: {OUT_JSON}")
    except Exception as e:
        print("ERROR writing aggregated JSON:", e)

    # stop printer
    await progress_q.put(None)
    await printer

# ------------------ CLI -------------------------
def parse_args():
    p = argparse.ArgumentParser(description="Targeted post-enricher for high-risk IOCs")
    p.add_argument("--min-score", type=int, default=70, help="Minimum score to enrich (default 70)")
    p.add_argument("--bucket", choices=["high","medium","low"], help="Or select by risk bucket (overrides min-score)")
    p.add_argument("--concurrency", type=int, default=20, help="Concurrency for HTTP workers (default 20)")
    p.add_argument("--limit", type=int, default=0, help="Limit number of targets (0 = no limit)")
    return p.parse_args()

def main():
    args = parse_args()
    min_score = args.min_score
    bucket = args.bucket
    limit = args.limit or None
    asyncio.run(main_async(min_score, bucket, args.concurrency, limit))

if __name__ == "__main__":
    main()
